{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import random\n",
    "from torch.utils.data.dataset import TensorDataset\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.optim import Adam\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from os.path import join\n",
    "from typing import Optional\n",
    "from tqdm.notebook import tqdm\n",
    "from scipy.stats import wasserstein_distance \n",
    "from matplotlib import pyplot as plt\n",
    "import threading, concurrent\n",
    "import glob\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "from qiskit import QuantumCircuit, QuantumRegister\n",
    "from qiskit import Aer, execute"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sys.path.append('../')\n",
    "\n",
    "class DatasetImages(Dataset): #\n",
    "    def __init__(self, data_type: str = 'train'):\n",
    "        with open('../data/images.npy', 'rb') as f:\n",
    "            images = np.load(f)\n",
    "        with open('../data/labels.npy', 'rb') as f:\n",
    "            labels = np.load(f)\n",
    "\n",
    "        # Scale the images to be between 0 and 2pi TODO: Chcek\n",
    "        images = (images - images.min()) / (images.max() - images.min()) * 2 * np.pi\n",
    "\n",
    "        # Convert to numpy\n",
    "        images = np.array(images, dtype=np.float32)\n",
    "        labels = np.array(labels, dtype=np.float32)\n",
    "\n",
    "\n",
    "        # images shape must be (n_samples, -1)\n",
    "        images = images.reshape(images.shape[0], -1)\n",
    "\n",
    "\n",
    "        # Split the data into train and validation\n",
    "        n_train = int(0.8 * len(images))\n",
    "        n_val = len(images) - n_train\n",
    "\n",
    "        if data_type == 'train':\n",
    "            images = images[:n_train]\n",
    "            labels = labels[:n_train]\n",
    "        elif data_type == 'val':\n",
    "            images = images[n_train:]\n",
    "            labels = labels[n_train:]\n",
    "\n",
    "        self.n_samples = len(images)\n",
    "        self.images = images\n",
    "        self.labels = labels\n",
    "\n",
    "\n",
    "        # Print shapes DEBUG\n",
    "        print(f\"Type: {data_type}\")\n",
    "        print(f\"Images shape: {self.images.shape}\")\n",
    "        print(f\"Labels shape: {self.labels.shape}\")\n",
    "\n",
    "\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return torch.from_numpy(self.images)[index], torch.from_numpy(self.labels)[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n_samples\n",
    "        \n",
    "    def all_items(self):\n",
    "        return torch.from_numpy(self.images), torch.from_numpy(self.labels)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoder(images):\n",
    "\n",
    "        output = []\n",
    "        for image in images:\n",
    "            # Initialize QC\n",
    "            q = QuantumRegister(16)\n",
    "            circuit = QuantumCircuit(q)\n",
    "\n",
    "            # Angle embedding\n",
    "            # Each data point is mapped to a rotation\n",
    "            # the rotation will alternate between the x, y and z axis\n",
    "            # Essentially each qubit will have the data of 3 pixels\n",
    "\n",
    "            total_features = 0 # Total features processed\n",
    "\n",
    "            while total_features < len(image):\n",
    "                # Lazy way to do it\n",
    "                for i in range(16):\n",
    "                    try:\n",
    "                        circuit.rx(image[total_features], q[i])\n",
    "                    except:\n",
    "                        total_features += 3\n",
    "                        break\n",
    "                    \n",
    "                    try:\n",
    "                        circuit.ry(image[total_features+1], q[i])\n",
    "                    except:\n",
    "                        total_features += 3\n",
    "                        break\n",
    "\n",
    "                    try:\n",
    "                        circuit.rz(image[total_features+2], q[i])\n",
    "                    except:\n",
    "                        total_features += 3\n",
    "                        break\n",
    "                    total_features += 3\n",
    "\n",
    "            # Measure and return the state\n",
    "            circuit.measure_all()\n",
    "\n",
    "            backend = Aer.get_backend('qasm_simulator')\n",
    "            job = execute(circuit, backend, shots=1000)\n",
    "            result = job.result()\n",
    "            counts = result.get_counts(circuit)\n",
    "\n",
    "            # Get the state with the biggest probability\n",
    "            state = max(counts, key=counts.get)\n",
    "            output.append(torch.tensor([int(i) for i in state], dtype=torch.float32)) # binary state\n",
    "\n",
    "\n",
    "        output = torch.stack(output)\n",
    "        #print(\"QC Output\", output.shape)\n",
    "        return output"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-> Decoder\n",
    "# Will receive an hidden state of 16 dimensions\n",
    "# will output a 28x28 image\n",
    "n_layers_encoder = 3\n",
    "layers = []\n",
    "hidden_size = 16\n",
    "\n",
    "# Input layer\n",
    "layers.append(nn.Linear(hidden_size, 16))\n",
    "layers.append(nn.ReLU())\n",
    "\n",
    "# Hidden layers\n",
    "for i in range(n_layers_encoder):\n",
    "    layers.append(nn.Linear(16, 16))\n",
    "    layers.append(nn.ReLU())\n",
    "\n",
    "# Output layer\n",
    "layers.append(nn.Linear(16, 28*28))\n",
    "layers.append(nn.Sigmoid())\n",
    "\n",
    "decoder = nn.Sequential(*layers)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode(x):\n",
    "    # Pass through encoder\n",
    "    return decoder(x)\n",
    "\n",
    "def forward(x):\n",
    "    # Pass through encoder\n",
    "    image_embedding = encoder(x)\n",
    "    \n",
    "    # Pass through decoder\n",
    "    output = decoder(image_embedding)\n",
    "\n",
    "    return output\n",
    "\n",
    "def training_step(batch, batch_idx):\n",
    "    x, _ = batch\n",
    "    \n",
    "    # Pass\n",
    "    output = forward(x)\n",
    "\n",
    "    # Loss\n",
    "    loss_criterion = nn.MSELoss()\n",
    "\n",
    "    # Print x and output shapes\n",
    "    #print(\"x\", x.shape)\n",
    "    #print(\"output\", output.shape)\n",
    "\n",
    "    loss = loss_criterion(x, output)\n",
    "\n",
    "    if loss.isnan().any():\n",
    "        raise KeyboardInterrupt\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define optimizer\n",
    "optimizer = Adam(decoder.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type: train\n",
      "Images shape: (1600, 784)\n",
      "Labels shape: (1600,)\n"
     ]
    }
   ],
   "source": [
    "# Define dataset\n",
    "dataset = DatasetImages('train')\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type: val\n",
      "Images shape: (400, 784)\n",
      "Labels shape: (400,)\n"
     ]
    }
   ],
   "source": [
    "val_dataset = DatasetImages('val').all_items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 - Val Loss: 8.987215042114258\n",
      "Epoch 0 - Loss: 7.314968109130859\n"
     ]
    }
   ],
   "source": [
    "# Train\n",
    "best_loss = 1e10\n",
    "for epoch in range(100):\n",
    "    for batch_idx, batch in enumerate(dataloader):\n",
    "        loss = training_step(batch, batch_idx)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # benchmark on validation set\n",
    "    with torch.no_grad():\n",
    "        val_loss = training_step(val_dataset, 0)\n",
    "        print(f'Epoch {epoch} - Val Loss: {val_loss.item()}')\n",
    "\n",
    "        if val_loss.item() < best_loss:\n",
    "            best_loss = val_loss.item()\n",
    "            torch.save(decoder.state_dict(), 'best_model.pt')\n",
    "\n",
    "    print(f'Epoch {epoch} - Loss: {loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3c92ef46ad2b2da948777c47db4b6416281773decc5eed7d27a54bd4d127e23d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
